## END TO END STUDENT PERFORMANCE INDICATOR PROJECT

## The Student Performance Indicator predicts student exam scores based on demographic, social, and academic features.This project demonstrates data preprocessing, feature engineering, model training, hyperparameter optimization, and deployment readiness.


ğŸ“ Student Performance Indicator
ğŸ“Š Predicting academic performance using Machine Learning

This project aims to predict student performance scores based on various demographic, social, and academic features using advanced regression models and hyperparameter tuning.

âœ… Final Model RÂ² Score: 87.77%

ğŸ§  Project Overview

The goal of this project is to build a complete end-to-end Machine Learning pipeline â€” from data ingestion and transformation to model training, evaluation, and deployment â€” for predicting student performance.

ğŸŒŸ Project Highlights

ğŸ”¹ Built a complete end-to-end Machine Learning pipeline

ğŸ”¹ Achieved RÂ² Score of 87.77% using Gradient Boosting Regressor

ğŸ”¹ Implemented 7 regression algorithms with GridSearchCV for hyperparameter tuning

ğŸ”¹ Designed a Flask-based API for predictions

ğŸ”¹ Fully modularized project with robust logging, exception handling, and artifact management


---

## ğŸ“ Repository Structure

```
Student-Performance-Indicator/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py         # Loads and splits the dataset
â”‚   â”‚   â”œâ”€â”€ data_transformation.py    # Preprocessing and feature scaling
â”‚   â”‚   â”œâ”€â”€ model_trainer.py          # Model training and evaluation
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ predict_pipeline.py       # For new predictions
â”‚   â”œâ”€â”€ utils.py                      # Helper functions (save/load objects)
â”‚   â”œâ”€â”€ exception.py                  # Custom exception handling
â”‚   â”œâ”€â”€ logger.py                     # Logging setup
â”‚
â”œâ”€â”€ artifacts/                        # Saved models and preprocessors
â”œâ”€â”€ reports/                          # Model performance reports
â”œâ”€â”€ notebooks/                        # EDA and experimentation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ app.py                            # Flask app for prediction
â””â”€â”€ README.md
```

---

## âš™ï¸ Workflow

1. Data Ingestion â†’ Read dataset and split into train/test
2. Data Transformation â†’ Encode, scale, and clean data
3. Model Training â†’ Train multiple regression models
4. Hyperparameter Tuning â†’ Optimize with `GridSearchCV`
5. Evaluation â†’ Compare based on **RÂ²**, **MAE**, **RMSE**
6. Model Saving â†’ Save the best model & preprocessor in `artifacts/`
7. Prediction Pipeline â†’ Load trained model to predict unseen data

---

## ğŸ¤– Models Implemented

| Algorithm                      | Description                             | Tuned | Library      |
| ------------------------------ | --------------------------------------- | ----- | ------------ |
| ğŸŒ² Random Forest Regressor     | Ensemble of Decision Trees              | âœ…     | scikit-learn |
| ğŸŒ³ Decision Tree Regressor     | Simple tree-based model                 | âœ…     | scikit-learn |
| ğŸš€ Gradient Boosting Regressor | Boosting with residual learning         | âœ…     | scikit-learn |
| ğŸ“ˆ Linear Regression           | Baseline linear model                   | âŒ     | scikit-learn |
| âš¡ XGBoost Regressor            | Gradient boosting with regularization   | âœ…     | xgboost      |
| ğŸ± CatBoost Regressor          | Boosting optimized for categorical data | âœ…     | catboost     |
| ğŸ”º AdaBoost Regressor          | Ensemble boosting using weak learners   | âœ…     | scikit-learn |

---

## ğŸ§® Hyperparameter Tuning

Each model was tuned with `GridSearchCV (cv=5)`:

```python

  models = {
                "Random Forest": RandomForestRegressor(),
                "Decision Tree": DecisionTreeRegressor(),
                "Gradient Boosting": GradientBoostingRegressor(),
                "Linear Regression": LinearRegression(),
                "XGBRegressor": XGBRegressor(),
                "CatBoosting Regressor": CatBoostRegressor(verbose=False),
                "AdaBoost Regressor": AdaBoostRegressor(),
            }
            params={
                "Decision Tree": {
                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],
                    'splitter':['best','random'],
                    'max_features':['sqrt','log2'],
                },
                "Random Forest":{
                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],
                 
                    'max_features':['sqrt','log2',None],
                    'n_estimators': [8,16,32,64,128,256]
                },
                "Gradient Boosting":{
                    'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],
                    'learning_rate':[.1,.01,.05,.001],
                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],
                    'criterion':['squared_error', 'friedman_mse'],
                    'max_features':['auto','sqrt','log2'],
                    'n_estimators': [8,16,32,64,128,256]
                },
                "Linear Regression":{},
                "XGBRegressor":{
                    'learning_rate':[.1,.01,.05,.001],
                    'n_estimators': [8,16,32,64,128,256]
                },
                "CatBoosting Regressor":{
                    'depth': [6,8,10],
                    'learning_rate': [0.01, 0.05, 0.1],
                    'iterations': [30, 50, 100]
                },
                "AdaBoost Regressor":{
                    'learning_rate':[.1,.01,0.5,.001],
                    'loss':['linear','square','exponential'],
                    'n_estimators': [8,16,32,64,128,256]
                }
                
            }
```

---

## ğŸ† Model Performance

| Model                           | RÂ² Score   | Status   |
| ------------------------------- | ---------- | -------- |
| Gradient Boosting Regressor     | 0.8777     | ğŸ† Best  |
| Random Forest Regressor         | 0.8612     | âœ…        |
| XGBoost Regressor               | 0.8564     | âœ…        |
| CatBoost Regressor              | 0.8501     | âœ…        |
| Decision Tree Regressor         | 0.8213     | âœ…        |
| AdaBoost Regressor              | 0.8085     | âœ…        |
| Linear Regression               | 0.7430     | Baseline |

> âœ… **Best Model Selected:** Gradient Boosting Regressor
> ğŸ“Š **Final RÂ² Score:** 87.77%

---

## ğŸ“ˆ Metrics Used

| Metric       | Description                              |
| ------------ | ---------------------------------------- |
| **RÂ² Score** | Measures variance explained by the model |
| **MAE**      | Mean Absolute Error                      |
| **MSE**      | Mean Squared Error                       |
| **RMSE**     | Root Mean Squared Error                  |

---

## ğŸ§° Tech Stack

| Category            | Tools                                          |
| ------------------- | ---------------------------------------------- |
| **Language**        | Python 3.8+                                    |
| **Libraries**       | Pandas, NumPy, Scikit-learn, XGBoost, CatBoost |
| **Deployment**      | Flask                                          |
| **Version Control** | Git & GitHub                                   |


---

## ğŸš€ Run Locally

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/sonali6062/Student-Performance-Indicator.git
cd Student-Performance-Indicator
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
conda create -p venv python==3.8 -y
conda activate venv/
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Train the Model

```bash
python src/components/model_trainer.py
```

### 5ï¸âƒ£ Run the Flask App

```bash
python app.py
```

Then open your browser and navigate to:
ğŸ‘‰ **[http://localhost:5000](http://localhost:5500)**

---

## ğŸ“¦ Artifacts Generated

| File                             | Description                    |
| -------------------------------- | ------------------------------ |
| `artifacts/model.pkl`            | Final trained regression model |
| `artifacts/preprocessor.pkl`     | Transformation pipeline        |
| `reports/final_model_report.csv` | Model performance summary      |
| `logs/`                          | System & training logs         |

---

## ğŸ§© Key Learnings

* Building modular ML pipelines for reusability
  
* Performing hyperparameter tuning effectively using GridSearchCV
  
* Understanding bias-variance tradeoff in regression models
  
* Saving and loading ML artifacts for deployment

---

## ğŸ› ï¸ Future Enhancements

* ğŸ“Š Add **feature importance** visualization
* 
* ğŸ§¾ Integrate **MLflow** for experiment tracking
* 
* ğŸŒ Build **Streamlit UI** for user-friendly predictions
* 
* âš™ï¸ Deploy on **Render / AWS EC2**

---

## ğŸ‘©â€ğŸ’» Author

**ğŸ‘¤ Sonali**
ğŸ’¡ Machine Learning Enthusiast | Data Science Learner
ğŸ”— [GitHub Profile](https://github.com/sonali6062)

---

---

## ğŸŒ  Summary 

ğŸ”¹ Developed a complete **regression-based ML pipeline** from scratch

ğŸ”¹ Achieved **87.77% RÂ² Score** with **Gradient Boosting**

ğŸ”¹ Implemented **hyperparameter tuning** for 7 models

ğŸ”¹ Built with **Flask**, **scikit-learn**, **XGBoost**, and **CatBoost**

ğŸ”¹ Demonstrates **real-world ML engineering workflow** (data â†’ model â†’ deployment)


